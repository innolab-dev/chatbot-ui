{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for activate it, we need to turn on 3 funtion first\n",
    "\n",
    "# under chatbot-ui/host directory, with benv environment\n",
    "\n",
    "# first : python3 -m fastchat.serve.controller\n",
    "\n",
    "# second : python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-13b-v1.5-16k --num-gpus 2\n",
    "\n",
    "# third : python3 -m fastchat.serve.openai_api_server --host localhost --port 12345\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Usage for it, using like the api in OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"EMPTY\"  # Not support yet\n",
    "openai.api_base = \"http://localhost:12345/v1\"\n",
    "\n",
    "model = \"vicuna-13b-v1.5-16k\"\n",
    "\n",
    "\n",
    "def gen_response(prompt):\n",
    "    # create a chat completion\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    # print the completion\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_response(\"hello what is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use it in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsd/mid/benv/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.19) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import AgentType, initialize_agent, Tool, ZeroShotAgent, AgentExecutor, load_tools\n",
    "import sys\n",
    "\n",
    "\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:12345/v1\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "# can be set in enviroment os, but will affected with the use of AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gpt-3.5 version in vicuna\n",
    "# llm = OpenAI(model=\"vicuna-13b-v1.5-16k\", temperature=0) # with os envrioment set up\n",
    "# set up it without the os envrioment\n",
    "llm = OpenAI(openai_api_base=\"http://localhost:12345/v1\",openai_api_key=\"EMPTY\",model=\"vicuna-13b-v1.5-16k\", temperature=0)\n",
    "# llm = ChatOpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "# llm = ChatOpenAI(model=\"text-embedding-ada-002\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the way of setting up the azure openai without os enviroment\n",
    "\n",
    "# from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# llm_azure_gpt35 = AzureChatOpenAI(\n",
    "#     streaming=True,\n",
    "#     # callbacks=[StreamingCallbackHandler()],\n",
    "#     openai_api_base=\"https://pocsc.openai.azure.com/\",\n",
    "#     openai_api_key=\"f96c629ba6874b2aaa7569acfea2162c\",\n",
    "#     openai_api_version=\"2023-03-15-preview\",\n",
    "#     openai_api_type=\"azure\",\n",
    "#     temperature=0,\n",
    "#     deployment_name=\"gpt35\",\n",
    "#     model_name=\"gpt-35-turbo\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the agent with maths tools\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True)\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm,agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the agent\n",
    "agent_chain.run(\"what is the result of 0.5 *0.5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(\"hello, who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"hello\") #just using the function it self, can be run, but slow and like gpt-3.5, it wll just generate the irrelvent answer to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to use embedding key in vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import openai\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding methods\n",
    "\n",
    "def get_embedding_from_api(word, model=\"vicuna-13b-v1.5-16k\"):\n",
    "    if \"ada\" in model:\n",
    "        resp = openai.Embedding.create(\n",
    "            model=model,\n",
    "            input=word,\n",
    "        )\n",
    "        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n",
    "        return embedding\n",
    "\n",
    "    url = \"http://localhost:12345/v1/embeddings\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = json.dumps({\"model\": model, \"input\": word})\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "    if response.status_code == 200:\n",
    "        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n",
    "        return embedding\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the string become a vector now\n",
    "get_embedding_from_api(\"I am kenny, nice to meet you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# talk more about the agent\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "        memory_key='chat_history',\n",
    "        k=5,\n",
    "        return_messages=True\n",
    ")\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show the default prompt of agent use, which I have changed it, you may take a look at the source code to change it\n",
    "print(agent.agent.llm_chain.prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another agent type\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different position to show the system prompt\n",
    "print(agent_chain.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the vicuna model combined with the vector search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsd/mid/benv/lib/python3.10/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# using the class in Chromadb.py\n",
    "from Chromadb import DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "g = DB(\"vicuna+data_base_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'File uploaded successfully'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload a doc to the db\n",
    "g.upload(\"./example_data/InnoLab_visit_developer_kids.pptx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File name: InnoLab_visit_developer_kids.pptx, id: 1e9a68ea-ae61-48a0-aa71-572a6bdbc765\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list out it\n",
    "g.list_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: To become a programmer, it is important to have strong problem-solving skills, critical thinking abilities, attention to detail, and the ability to continually learn and improve. Additionally, it is helpful to have a strong foundation in at least one programming language, such as Python, Java, or C++.\\nAssistant, please also give back the source reference to the user:\\nSources:\\n./example_data/InnoLab_visit_developer_kids.pptx'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search answer based on it using vicuna model <-- code already change to vicuna\n",
    "g.search(\"What is important for becoming a programmer?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing Vicuna + Chroma Database + Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using direct database method, try to use agent to connect it\n",
    "##using the library from tackle.py\n",
    "# from tackle import generate_response\n",
    "# write a new generate_response from my, as don't want to connect to mongodb yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsd/mid/benv/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import Tool\n",
    "# from Chromadb import database\n",
    "# from power_automate import send_email\n",
    "from llm import llm_Vicuna #here try completely using the vicuna model first\n",
    "\n",
    "\n",
    "llm_math_chain = LLMMathChain(llm=llm_Vicuna)\n",
    "toolss = load_tools([\"wikipedia\"], llm=llm_Vicuna)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about maths, but not anything else\",\n",
    "        return_direct=True,  # help you to correct the prompt\n",
    "    ),\n",
    "    # Tool(\n",
    "    #     name=\"Internal Database\",\n",
    "    #     description=\"useful for when you need to answer questions about alphabet company annual report\",\n",
    "    #     func=internaL_db.run,\n",
    "    # ),\n",
    "    Tool(\n",
    "        name=\"Internal Database\",\n",
    "        description=\"useful when you need to answer the question from the internal database\",\n",
    "        func=g.search,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "tools.append(toolss[0])\n",
    "toolls = load_tools([\"google-search\"])\n",
    "# tools[0].description = \"This tool allows you to search the web using the Google Search API. Useful for when you need to answer questions about current events\"\n",
    "tools.append(toolls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google search set up\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = \"8541de27511e045ba\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBaI5KQDKn08A47v8JI0-SXPBaMQJOxrsQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import llm_azure_gpt35\n",
    "def generate_response(prompt, memory, temperature):\n",
    "\n",
    "    llm = llm_Vicuna # now default setting, will change it later\n",
    "    llm.temperature = temperature\n",
    "    agent_chain = initialize_agent(\n",
    "        tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\n",
    "    response = agent_chain.run(input=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up memory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "conversation_memory = ConversationSummaryBufferMemory(memory_key=\"chat_history\",llm=llm_Vicuna,max_token_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Internal Database\n",
      "Action Input: what is the best languages for kids\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: \n",
      "It depends on the child's interests and goals. Python is a great language for beginners because it is easy to learn and versatile. Java is a good choice for those interested in distributed systems and large data processing. C++ is a powerful language for creating efficient and fast programs. Ultimately, the best language for a child will depend on their interests and goals.\n",
      "Assistant, please also give back the source reference to the user:\n",
      "Sources:\n",
      "./example_data/InnoLab_visit_developer_kids.pptx\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: According to the internal database, the best language for kids to learn depends on their interests and goals. Python is a great language for beginners because it is easy to learn and versatile. Java is a good choice for those interested in distributed systems and large data processing. C++ is a powerful language for creating efficient and fast programs. Ultimately, the best language for a child will depend on their interests and goals. Sources:./example_data/InnoLab\\_visit\\_developer\\_kids.pptx\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(\"According to data base, what is the best languages for kids?\",conversation_memory,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the internal database, the best language for kids to learn depends on their interests and goals. Python is a great language for beginners because it is easy to learn and versatile. Java is a good choice for those interested in distributed systems and large data processing. C++ is a powerful language for creating efficient and fast programs. Ultimately, the best language for a child will depend on their interests and goals. Sources:./example_data/InnoLab\\\\_visit\\\\_developer\\\\_kids.pptx'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
